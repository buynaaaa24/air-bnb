import os
from dotenv import load_dotenv
from openai import OpenAI
from sentence_transformers import SentenceTransformer
from transformers import AutoModelForCausalLM, AutoTokenizer
import torch
import psycopg2
import json

# –û—Ä—á–Ω—ã —Ö—É–≤—å—Å–∞–≥—á–∏–π–≥ –∞—á–∞–∞–ª–∞—Ö
load_dotenv()
API_KEY = os.getenv("OPENAI_API_KEY")

if not API_KEY:
    raise ValueError("API —Ç“Ø–ª—Ö“Ø“Ø—Ä –æ–ª–¥—Å–æ–Ω–≥“Ø–π. .env —Ñ–∞–π–ª–¥ OPENAI_API_KEY —Ç–æ—Ö–∏—Ä—É—É–ª–∞–≥–¥—Å–∞–Ω —ç—Å—ç—Ö–∏–π–≥ —à–∞–ª–≥–∞–Ω–∞ —É—É.")

OpenAI.api_key = API_KEY

# –ó–∞–≥–≤–∞—Ä—É—É–¥—ã–Ω —Ç–æ—Ö–∏—Ä–≥–æ–æ
embedding_model = SentenceTransformer("nomic-ai/nomic-embed-text-v1.5")
tokenizer = AutoTokenizer.from_pretrained("facebook/opt-350m")

offload_folder = r"C:\\Users\\abuyn\\offload"
model = AutoModelForCausalLM.from_pretrained(
    r"C:\\Users\\abuyn\\.cache\\huggingface\\hub\\models--facebook--opt-350m\\snapshots\\08ab08cc4b72ff5593870b5d527cf4230323703c",
    torch_dtype=torch.float16,
    device_map="auto",
    offload_folder=offload_folder,
)

# ”®–≥”©–≥–¥–ª–∏–π–Ω —Å–∞–Ω—Ç–∞–π —Ö–æ–ª–±–æ–≥–¥–æ—Ö
conn = psycopg2.connect(
    dbname="odoo_db",
    user="odoo18",
    password="odoo",
    host="localhost",
    port="5432"
)
cursor = conn.cursor()


# üìå **–•–∞–º–≥–∏–π–Ω –∏—Ö –±–æ—Ä–ª—É—É–ª–∞–ª—Ç—Ç–∞–π –±“Ø—Ç—ç—ç–≥–¥—ç—Ö“Ø“Ø–Ω –∞–≤–∞—Ö**
def get_sales_data():
    cursor.execute("""
        SELECT product_name, SUM(amount) AS total_sales
        FROM sales
        GROUP BY product_name
        ORDER BY total_sales DESC
        LIMIT 10;
    """)
    rows = cursor.fetchall()
    return [{"product_name": row[0], "total_sales": row[1]} for row in rows]


# üìå **–•–∞–º–≥–∏–π–Ω —Ö—è–º–¥ –±“Ø—Ç—ç—ç–≥–¥—ç—Ö“Ø“Ø–Ω –∞–≤–∞—Ö**
def get_cheapest_product():
    try:
        cursor.execute("""
            SELECT product_name, price
            FROM product_sales
            ORDER BY price ASC
            LIMIT 1;
        """)
        row = cursor.fetchone()
        if row:
            return f"–•–∞–º–≥–∏–π–Ω —Ö—è–º–¥ –±“Ø—Ç—ç—ç–≥–¥—ç—Ö“Ø“Ø–Ω: '{row[0]}' –Ω—å ${row[1]:.2f}-–∏–π–Ω “Ø–Ω—ç—Ç—ç–π."
        else:
            return "–ë“Ø—Ç—ç—ç–≥–¥—ç—Ö“Ø“Ø–Ω–∏–π –º—ç–¥—ç—ç–ª—ç–ª –æ–ª–¥—Å–æ–Ω–≥“Ø–π."
    except Exception as e:
        print(f"–ê–ª–¥–∞–∞: {e}")
        return "–•–∞–º–≥–∏–π–Ω —Ö—è–º–¥ –±“Ø—Ç—ç—ç–≥–¥—ç—Ö“Ø“Ø–Ω–∏–π –º—ç–¥—ç—ç–ª–ª–∏–π–≥ –∞–≤–∞—Ö–∞–¥ –∞–ª–¥–∞–∞ –≥–∞—Ä–ª–∞–∞."


# üìå **–°–∞–Ω—Ö“Ø“Ø–≥–∏–π–Ω —Ç–∞–π–ª–∞–Ω –≥–∞—Ä–≥–∞—Ö**
def get_financial_report():
    try:
        cursor.execute("""
            SELECT product_name, SUM(quantity) AS total_sold, SUM(quantity * price) AS total_revenue
            FROM product_sales
            GROUP BY product_name
            ORDER BY total_revenue DESC;
        """)
        rows = cursor.fetchall()–°–∞–Ω—Ö“Ø“Ø–≥–∏–π–Ω –±“Ø—Ö —Ç–∞–π–ª–∞–Ω–≥ –≥–∞—Ä–≥–∞–∂ ”©–≥”©”©—á.

        if not rows:
            return "–ë–æ—Ä–ª—É—É–ª–∞–ª—Ç—ã–Ω –º—ç–¥—ç—ç–ª—ç–ª –æ–ª–¥—Å–æ–Ω–≥“Ø–π."

        report = "üìä **–°–∞–Ω—Ö“Ø“Ø–≥–∏–π–Ω —Ç–∞–π–ª–∞–Ω** üìä\n"
        total_earnings = 0

        for row in rows:
            product_name, total_sold, total_revenue = row
            total_earnings += total_revenue
            report += f"- {product_name}: {total_sold} —à–∏—Ä—Ö—ç–≥, –Ω–∏–π—Ç “Ø–Ω—ç: ${total_revenue:.2f}\n"

        report += f"\nüí∞ **–ù–∏–π—Ç –æ—Ä–ª–æ–≥–æ:** ${total_earnings:.2f}"
        return report

    except Exception as e:
        print(f"–ê–ª–¥–∞–∞ –≥–∞—Ä–ª–∞–∞: {e}")
        return "–°–∞–Ω—Ö“Ø“Ø–≥–∏–π–Ω —Ç–∞–π–ª–∞–Ω –≥–∞—Ä–≥–∞—Ö–∞–¥ –∞–ª–¥–∞–∞ –≥–∞—Ä–ª–∞–∞."


# üìå **”®–≥”©–≥–¥–ª–∏–π–Ω —Å–∞–Ω–≥–∏–π–Ω –∞—Å—É—É–ª—Ç—É—É–¥—ã–≥ –æ–π–ª–≥–æ—Ö**
def analyze_context(sentence):
    topics = "–ë—É—Å–∞–¥"
    summary = ""
    sentence = sentence.lower()

    if any(keyword in sentence for keyword in ["—Ö–∞–º–≥–∏–π–Ω –∏—Ö –±–æ—Ä–ª—É—É–ª–∞–ª—Ç", "—à–∏–ª–¥—ç–≥ –±“Ø—Ç—ç—ç–≥–¥—ç—Ö“Ø“Ø–Ω"]):
        sales_data = get_sales_data()
        summary = "–•–∞–º–≥–∏–π–Ω –∏—Ö –±–æ—Ä–ª—É—É–ª–∞–ª—Ç—Ç–∞–π –±“Ø—Ç—ç—ç–≥–¥—ç—Ö“Ø“Ø–Ω“Ø“Ø–¥:\n" + "\n".join(
            [f"{product['product_name']} - ${product['total_sales']:.2f}" for product in sales_data])
        topics = "–ë–æ—Ä–ª—É—É–ª–∞–ª—Ç"
    elif "—Ö–∞–º–≥–∏–π–Ω —Ö—è–º–¥" in sentence:
        summary = get_cheapest_product()
        topics = "–ë“Ø—Ç—ç—ç–≥–¥—ç—Ö“Ø“Ø–Ω"
    elif "—Å–∞–Ω—Ö“Ø“Ø–≥–∏–π–Ω –±“Ø—Ö —Ç–∞–π–ª–∞–Ω" in sentence:
        summary = get_financial_report()
        topics = "–°–∞–Ω—Ö“Ø“Ø"
    else:
        summary = "–¢–æ—Ö–∏—Ä–æ—Ö –º—ç–¥—ç—ç–ª—ç–ª –æ–ª–¥—Å–æ–Ω–≥“Ø–π."

    cursor.execute("INSERT INTO context_analysis (sentence, topics) VALUES (%s, %s)", (sentence, topics))
    conn.commit()

    return {"summary": summary}


# üìå **–ß–∞—Ç –±–æ—Ç—ã–Ω “Ø–Ω–¥—Å—ç–Ω —Ñ—É–Ω–∫—Ü**
def interactive_conversation():
    print("–°–∞–π–Ω –±–∞–π–Ω–∞ —É—É! –¢–∞–Ω–¥ —é—É–≥–∞–∞—Ä —Ç—É—Å–ª–∞—Ö –≤—ç? ('–≥–∞—Ä–∞—Ö' –≥—ç–∂ –±–∏—á–∏–∂ –¥—É—É—Å–≥–∞–Ω–∞ —É—É.)")
    while True:
        user_input = input("–¢–∞: ").strip()
        if user_input.lower() == '–≥–∞—Ä–∞—Ö':
            print("–ë–∞—è—Ä—Ç–∞–π!")
            break
        result = analyze_context(user_input)
        print(f"–ë–æ—Ç: {result['summary']}")


# –ü—Ä–æ–≥—Ä–∞–º—ã–≥ –∞–∂–∏–ª–ª—É—É–ª–∞—Ö
if __name__ == "__main__":
    interactive_conversation()

cursor.close()
conn.close()
